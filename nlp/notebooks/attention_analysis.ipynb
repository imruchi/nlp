{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mechanistic Interpretability: Attention Head Analysis\n",
    "\n",
    "This notebook focuses on analyzing attention patterns in the Qwen2-7B model during financial statement analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jnk789/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# For better visualization\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Setup with Attention Hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionExtractor:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.attention_weights = {}\n",
    "        self.hooks = []\n",
    "        \n",
    "    def register_hooks(self):\n",
    "        \"\"\"Register hooks to extract attention weights from all layers\"\"\"\n",
    "        def get_attention_hook(layer_idx):\n",
    "            def hook(module, input, output):\n",
    "                # For Qwen2, attention weights are in output.attentions\n",
    "                if hasattr(output, 'attentions') and output.attentions is not None:\n",
    "                    self.attention_weights[f'layer_{layer_idx}'] = output.attentions.detach().cpu()\n",
    "                elif len(output) > 1 and output[1] is not None:\n",
    "                    # Sometimes attention is the second element in output tuple\n",
    "                    self.attention_weights[f'layer_{layer_idx}'] = output[1].detach().cpu()\n",
    "            return hook\n",
    "        \n",
    "        # Register hooks for all transformer layers\n",
    "        for i, layer in enumerate(self.model.model.layers):\n",
    "            hook = layer.self_attn.register_forward_hook(get_attention_hook(i))\n",
    "            self.hooks.append(hook)\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Remove all registered hooks\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "    \n",
    "    def clear_attention_weights(self):\n",
    "        \"\"\"Clear stored attention weights\"\"\"\n",
    "        self.attention_weights = {}\n",
    "    \n",
    "    def generate_with_attention(self, text: str, max_new_tokens: int = 512, **generation_kwargs):\n",
    "        \"\"\"Generate text while capturing attention weights\"\"\"\n",
    "        self.clear_attention_weights()\n",
    "        self.register_hooks()\n",
    "        \n",
    "        try:\n",
    "            # Prepare input\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.model.device)\n",
    "            input_length = inputs.input_ids.shape[1]\n",
    "            \n",
    "            # Generate with attention output\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    output_attentions=True,\n",
    "                    return_dict_in_generate=True,\n",
    "                    **generation_kwargs\n",
    "                )\n",
    "            \n",
    "            # Decode response\n",
    "            generated_text = self.tokenizer.decode(\n",
    "                outputs.sequences[0][input_length:], \n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            \n",
    "            # Get tokens for analysis\n",
    "            all_tokens = self.tokenizer.convert_ids_to_tokens(outputs.sequences[0])\n",
    "            \n",
    "            return {\n",
    "                'generated_text': generated_text,\n",
    "                'input_tokens': all_tokens[:input_length],\n",
    "                'output_tokens': all_tokens[input_length:],\n",
    "                'all_tokens': all_tokens,\n",
    "                'attention_weights': self.attention_weights.copy(),\n",
    "                'input_length': input_length\n",
    "            }\n",
    "            \n",
    "        finally:\n",
    "            self.remove_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with reduced memory usage\n",
    "model_name = \"Qwen/Qwen2-7B-Instruct\"\n",
    "\n",
    "# Use CPU or smaller model if MPS memory is insufficient\n",
    "device = \"cpu\"  # Change to \"mps\" if you have enough memory\n",
    "\n",
    "print(f\"Loading model on {device}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if device != \"cpu\" else torch.float32,\n",
    "    device_map=\"auto\" if device != \"cpu\" else None,\n",
    ")\n",
    "\n",
    "if device == \"cpu\":\n",
    "    model = model.to(device)\n",
    "\n",
    "# Initialize attention extractor\n",
    "attention_extractor = AttentionExtractor(model, tokenizer)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Financial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and prompts\n",
    "with open('../data/process_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(\"../config/prompts.yaml\", \"r\") as f:\n",
    "    prompts = yaml.safe_load(f)\n",
    "\n",
    "# Select a sample for analysis\n",
    "sample_key = '1000'\n",
    "sample_year = '1973.0'\n",
    "sample_data = data[sample_key][sample_year]\n",
    "\n",
    "print(f\"Sample company: {sample_key}, Year: {sample_year}\")\n",
    "print(f\"Label: {sample_data['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Response with Attention Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the prompt\n",
    "prompt_input = prompts['prompt_1'][0].format(balance_income_sheet=sample_data['description'])\n",
    "\n",
    "# Format for chat template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt_input}]\n",
    "formatted_text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(\"Input length:\", len(tokenizer.encode(formatted_text)))\n",
    "print(\"\\nGenerating response with attention capture...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with attention tracking (use shorter generation for analysis)\n",
    "result = attention_extractor.generate_with_attention(\n",
    "    formatted_text, \n",
    "    max_new_tokens=100,  # Shorter for faster analysis\n",
    "    do_sample=False,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"Generated text:\")\n",
    "print(result['generated_text'])\n",
    "print(f\"\\nCaptured attention from {len(result['attention_weights'])} layers\")\n",
    "print(f\"Input tokens: {len(result['input_tokens'])}\")\n",
    "print(f\"Output tokens: {len(result['output_tokens'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Attention Analysis Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionAnalyzer:\n",
    "    def __init__(self, attention_weights, tokens, input_length):\n",
    "        self.attention_weights = attention_weights\n",
    "        self.tokens = tokens\n",
    "        self.input_length = input_length\n",
    "        self.num_layers = len(attention_weights)\n",
    "        \n",
    "    def get_attention_stats(self):\n",
    "        \"\"\"Get basic statistics about attention patterns\"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        for layer_name, attn in self.attention_weights.items():\n",
    "            if attn is not None and len(attn.shape) >= 3:\n",
    "                # attn shape: [batch, heads, seq_len, seq_len]\n",
    "                stats[layer_name] = {\n",
    "                    'shape': list(attn.shape),\n",
    "                    'num_heads': attn.shape[1] if len(attn.shape) > 1 else 1,\n",
    "                    'seq_length': attn.shape[-1] if len(attn.shape) > 0 else 0,\n",
    "                    'mean_attention': float(attn.mean()),\n",
    "                    'max_attention': float(attn.max()),\n",
    "                    'attention_entropy': self._calculate_entropy(attn)\n",
    "                }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def _calculate_entropy(self, attention_matrix):\n",
    "        \"\"\"Calculate entropy of attention distribution\"\"\"\n",
    "        # Add small epsilon to avoid log(0)\n",
    "        eps = 1e-10\n",
    "        attention_matrix = attention_matrix + eps\n",
    "        entropy = -(attention_matrix * torch.log(attention_matrix)).sum(dim=-1).mean()\n",
    "        return float(entropy)\n",
    "    \n",
    "    def plot_attention_heatmap(self, layer_idx=0, head_idx=0, max_tokens=50):\n",
    "        \"\"\"Plot attention heatmap for a specific layer and head\"\"\"\n",
    "        layer_name = f'layer_{layer_idx}'\n",
    "        \n",
    "        if layer_name not in self.attention_weights:\n",
    "            print(f\"Layer {layer_idx} not found in attention weights\")\n",
    "            return\n",
    "        \n",
    "        attn = self.attention_weights[layer_name]\n",
    "        if attn is None or len(attn.shape) < 4:\n",
    "            print(f\"Invalid attention tensor for layer {layer_idx}\")\n",
    "            return\n",
    "        \n",
    "        # Extract attention for specific head\n",
    "        attention_matrix = attn[0, head_idx].numpy()  # [seq_len, seq_len]\n",
    "        \n",
    "        # Limit tokens for visualization\n",
    "        seq_len = min(attention_matrix.shape[0], max_tokens)\n",
    "        attention_matrix = attention_matrix[:seq_len, :seq_len]\n",
    "        display_tokens = self.tokens[:seq_len]\n",
    "        \n",
    "        # Create heatmap\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(\n",
    "            attention_matrix,\n",
    "            xticklabels=[f\"{i}:{token}\" for i, token in enumerate(display_tokens)],\n",
    "            yticklabels=[f\"{i}:{token}\" for i, token in enumerate(display_tokens)],\n",
    "            cmap='Blues',\n",
    "            cbar=True\n",
    "        )\n",
    "        plt.title(f'Attention Heatmap - Layer {layer_idx}, Head {head_idx}')\n",
    "        plt.xlabel('Key Position')\n",
    "        plt.ylabel('Query Position')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_attention_heads_summary(self, layer_idx=0):\n",
    "        \"\"\"Plot summary of all attention heads in a layer\"\"\"\n",
    "        layer_name = f'layer_{layer_idx}'\n",
    "        \n",
    "        if layer_name not in self.attention_weights:\n",
    "            print(f\"Layer {layer_idx} not found\")\n",
    "            return\n",
    "        \n",
    "        attn = self.attention_weights[layer_name]\n",
    "        if attn is None or len(attn.shape) < 4:\n",
    "            print(f\"Invalid attention tensor for layer {layer_idx}\")\n",
    "            return\n",
    "        \n",
    "        num_heads = attn.shape[1]\n",
    "        cols = min(4, num_heads)\n",
    "        rows = (num_heads + cols - 1) // cols\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(15, 4*rows))\n",
    "        if rows == 1:\n",
    "            axes = [axes] if cols == 1 else axes\n",
    "        else:\n",
    "            axes = axes.flatten()\n",
    "        \n",
    "        for head_idx in range(num_heads):\n",
    "            attention_matrix = attn[0, head_idx].numpy()\n",
    "            \n",
    "            # Focus on last few tokens (where generation happens)\n",
    "            seq_len = min(30, attention_matrix.shape[0])\n",
    "            start_idx = max(0, attention_matrix.shape[0] - seq_len)\n",
    "            \n",
    "            sub_attn = attention_matrix[start_idx:, start_idx:]\n",
    "            \n",
    "            ax = axes[head_idx] if num_heads > 1 else axes[0]\n",
    "            sns.heatmap(sub_attn, ax=ax, cmap='Blues', cbar=True)\n",
    "            ax.set_title(f'Head {head_idx}')\n",
    "            ax.set_xlabel('Key Position')\n",
    "            ax.set_ylabel('Query Position')\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for idx in range(num_heads, len(axes)):\n",
    "            axes[idx].set_visible(False)\n",
    "        \n",
    "        plt.suptitle(f'All Attention Heads - Layer {layer_idx}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_financial_attention(self):\n",
    "        \"\"\"Analyze attention patterns specific to financial terms\"\"\"\n",
    "        # Financial keywords to look for\n",
    "        financial_terms = [\n",
    "            'sales', 'revenue', 'income', 'profit', 'loss', 'asset', 'liability',\n",
    "            'equity', 'cash', 'debt', 'eps', 'earnings', 'balance', 'sheet',\n",
    "            'statement', 'financial', 'increase', 'decrease', 'ratio'\n",
    "        ]\n",
    "        \n",
    "        # Find positions of financial terms\n",
    "        financial_positions = []\n",
    "        for i, token in enumerate(self.tokens):\n",
    "            if any(term in token.lower() for term in financial_terms):\n",
    "                financial_positions.append((i, token))\n",
    "        \n",
    "        print(f\"Found {len(financial_positions)} financial terms:\")\n",
    "        for pos, token in financial_positions[:10]:  # Show first 10\n",
    "            print(f\"  Position {pos}: {token}\")\n",
    "        \n",
    "        return financial_positions\n",
    "    \n",
    "    def plot_attention_to_financial_terms(self, financial_positions, layer_idx=0, head_idx=0):\n",
    "        \"\"\"Plot attention from generated tokens to financial terms in input\"\"\"\n",
    "        layer_name = f'layer_{layer_idx}'\n",
    "        \n",
    "        if layer_name not in self.attention_weights or not financial_positions:\n",
    "            print(\"No data available for analysis\")\n",
    "            return\n",
    "        \n",
    "        attn = self.attention_weights[layer_name]\n",
    "        if attn is None:\n",
    "            return\n",
    "        \n",
    "        attention_matrix = attn[0, head_idx].numpy()\n",
    "        \n",
    "        # Get financial term positions\n",
    "        fin_positions = [pos for pos, _ in financial_positions if pos < attention_matrix.shape[1]]\n",
    "        \n",
    "        # Focus on output tokens (generated part)\n",
    "        output_start = self.input_length\n",
    "        if output_start < attention_matrix.shape[0]:\n",
    "            # Attention from output tokens to financial terms\n",
    "            output_to_financial = attention_matrix[output_start:, fin_positions]\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.heatmap(\n",
    "                output_to_financial,\n",
    "                xticklabels=[f\"{pos}:{self.tokens[pos]}\" for pos in fin_positions],\n",
    "                yticklabels=[f\"{i+output_start}:{self.tokens[i+output_start]}\" \n",
    "                           for i in range(min(20, output_to_financial.shape[0]))],\n",
    "                cmap='Reds'\n",
    "            )\n",
    "            plt.title(f'Generated Tokens Attention to Financial Terms\\nLayer {layer_idx}, Head {head_idx}')\n",
    "            plt.xlabel('Financial Terms in Input')\n",
    "            plt.ylabel('Generated Tokens')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = AttentionAnalyzer(\n",
    "    result['attention_weights'], \n",
    "    result['all_tokens'], \n",
    "    result['input_length']\n",
    ")\n",
    "\n",
    "print(\"Attention analyzer initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Basic Attention Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attention statistics\n",
    "stats = analyzer.get_attention_stats()\n",
    "\n",
    "print(\"Attention Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "for layer_name, layer_stats in stats.items():\n",
    "    print(f\"\\n{layer_name}:\")\n",
    "    print(f\"  Shape: {layer_stats['shape']}\")\n",
    "    print(f\"  Heads: {layer_stats['num_heads']}\")\n",
    "    print(f\"  Sequence Length: {layer_stats['seq_length']}\")\n",
    "    print(f\"  Mean Attention: {layer_stats['mean_attention']:.4f}\")\n",
    "    print(f\"  Max Attention: {layer_stats['max_attention']:.4f}\")\n",
    "    print(f\"  Attention Entropy: {layer_stats['attention_entropy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Attention Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot attention heatmap for a specific layer and head\n",
    "analyzer.plot_attention_heatmap(layer_idx=0, head_idx=0, max_tokens=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot attention heatmap for a middle layer\n",
    "middle_layer = analyzer.num_layers // 2\n",
    "analyzer.plot_attention_heatmap(layer_idx=middle_layer, head_idx=0, max_tokens=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all attention heads for a specific layer\n",
    "analyzer.plot_attention_heads_summary(layer_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Financial-Specific Attention Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze attention to financial terms\n",
    "financial_positions = analyzer.analyze_financial_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot attention from generated tokens to financial terms\n",
    "analyzer.plot_attention_to_financial_terms(financial_positions, layer_idx=0, head_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different layers\n",
    "analyzer.plot_attention_to_financial_terms(financial_positions, layer_idx=middle_layer, head_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Analysis: Head Specialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_head_specialization(analyzer, layer_idx=0):\n",
    "    \"\"\"Analyze what different attention heads focus on\"\"\"\n",
    "    layer_name = f'layer_{layer_idx}'\n",
    "    \n",
    "    if layer_name not in analyzer.attention_weights:\n",
    "        print(f\"Layer {layer_idx} not found\")\n",
    "        return\n",
    "    \n",
    "    attn = analyzer.attention_weights[layer_name]\n",
    "    if attn is None:\n",
    "        return\n",
    "    \n",
    "    num_heads = attn.shape[1]\n",
    "    \n",
    "    print(f\"\\nHead Specialization Analysis - Layer {layer_idx}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for head_idx in range(min(8, num_heads)):  # Analyze first 8 heads\n",
    "        attention_matrix = attn[0, head_idx].numpy()\n",
    "        \n",
    "        # Calculate attention patterns\n",
    "        diagonal_attention = np.diag(attention_matrix).mean()\n",
    "        \n",
    "        # Attention to previous tokens (causal pattern)\n",
    "        causal_mask = np.tril(np.ones_like(attention_matrix), k=-1)\n",
    "        causal_attention = (attention_matrix * causal_mask).sum() / causal_mask.sum()\n",
    "        \n",
    "        # Attention to beginning of sequence\n",
    "        beginning_attention = attention_matrix[:, :10].mean()\n",
    "        \n",
    "        # Attention spread (entropy)\n",
    "        entropy = -(attention_matrix * np.log(attention_matrix + 1e-10)).sum(axis=1).mean()\n",
    "        \n",
    "        print(f\"\\nHead {head_idx}:\")\n",
    "        print(f\"  Diagonal (self) attention: {diagonal_attention:.4f}\")\n",
    "        print(f\"  Causal (previous) attention: {causal_attention:.4f}\")\n",
    "        print(f\"  Beginning attention: {beginning_attention:.4f}\")\n",
    "        print(f\"  Attention entropy: {entropy:.4f}\")\n",
    "        \n",
    "        # Determine head type\n",
    "        if diagonal_attention > 0.3:\n",
    "            head_type = \"Self-attention head\"\n",
    "        elif beginning_attention > causal_attention:\n",
    "            head_type = \"Beginning-focused head\"\n",
    "        elif entropy < 2.0:\n",
    "            head_type = \"Focused attention head\"\n",
    "        else:\n",
    "            head_type = \"Distributed attention head\"\n",
    "        \n",
    "        print(f\"  Type: {head_type}\")\n",
    "\n",
    "# Analyze different layers\n",
    "analyze_head_specialization(analyzer, layer_idx=0)\n",
    "analyze_head_specialization(analyzer, layer_idx=middle_layer)\n",
    "analyze_head_specialization(analyzer, layer_idx=analyzer.num_layers-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Token-to-Token Attention Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_specific_token_attention(analyzer, target_token_text, layer_idx=0, head_idx=0):\n",
    "    \"\"\"Analyze what a specific token attends to\"\"\"\n",
    "    # Find token position\n",
    "    target_positions = []\n",
    "    for i, token in enumerate(analyzer.tokens):\n",
    "        if target_token_text.lower() in token.lower():\n",
    "            target_positions.append(i)\n",
    "    \n",
    "    if not target_positions:\n",
    "        print(f\"Token '{target_token_text}' not found\")\n",
    "        return\n",
    "    \n",
    "    layer_name = f'layer_{layer_idx}'\n",
    "    if layer_name not in analyzer.attention_weights:\n",
    "        print(f\"Layer {layer_idx} not found\")\n",
    "        return\n",
    "    \n",
    "    attn = analyzer.attention_weights[layer_name]\n",
    "    if attn is None:\n",
    "        return\n",
    "    \n",
    "    attention_matrix = attn[0, head_idx].numpy()\n",
    "    \n",
    "    print(f\"\\nAttention analysis for '{target_token_text}'\")\n",
    "    print(f\"Found at positions: {target_positions}\")\n",
    "    \n",
    "    for pos in target_positions[:3]:  # Analyze first 3 occurrences\n",
    "        if pos < attention_matrix.shape[0]:\n",
    "            attention_weights = attention_matrix[pos]\n",
    "            \n",
    "            # Get top attending tokens\n",
    "            top_indices = np.argsort(attention_weights)[-10:][::-1]\n",
    "            \n",
    "            print(f\"\\nPosition {pos} ('{analyzer.tokens[pos]}') attends most to:\")\n",
    "            for i, idx in enumerate(top_indices):\n",
    "                if idx < len(analyzer.tokens):\n",
    "                    print(f\"  {i+1}. Position {idx}: '{analyzer.tokens[idx]}' (weight: {attention_weights[idx]:.4f})\")\n",
    "\n",
    "# Analyze attention for key financial terms\n",
    "analyze_specific_token_attention(analyzer, \"sales\", layer_idx=0, head_idx=0)\n",
    "analyze_specific_token_attention(analyzer, \"increase\", layer_idx=middle_layer, head_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mechanistic Interpretability Analysis Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Analyzed layers: {len(result['attention_weights'])}\")\n",
    "print(f\"Input tokens: {len(result['input_tokens'])}\")\n",
    "print(f\"Generated tokens: {len(result['output_tokens'])}\")\n",
    "print(f\"Financial terms found: {len(financial_positions)}\")\n",
    "\n",
    "print(\"\\nNext Steps for Deeper Analysis:\")\n",
    "print(\"1. Compare attention patterns across different financial scenarios\")\n",
    "print(\"2. Analyze how attention changes for 'increase' vs 'decrease' predictions\")\n",
    "print(\"3. Study attention patterns in different layers (early vs late)\")\n",
    "print(\"4. Investigate head specialization across the full model\")\n",
    "print(\"5. Correlate attention patterns with prediction accuracy\")\n",
    "print(\"6. Create attention interventions to test causal relationships\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
